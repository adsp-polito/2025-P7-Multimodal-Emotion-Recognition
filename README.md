# 2025-P7-Multimodal-Emotion-Recognition

---

## Overview

This repository contains the complete work for the Multimodal Emotion Recognition (MER) project, focused on detecting emotional states from speech using multimodal large language models such as Qwen2-Audio and Voxtral. The project explores both zero-shot and parameter-efficient fine-tuning strategies to classify emotions in health-oriented scenarios. Our system is designed to support therapists, researchers, and patients by providing interpretable emotional insights derived from real speech data.

This work follows the four-pillar methodology established by LINKS Foundation: **Design, Manage, Develop, Communicate**.

---

## Objectives

- Build a multimodal pipeline capable of processing speech audio through an audio-language model.
- Evaluate zero-shot performance on emotion classification tasks.
- Implement parameter-efficient fine-tuning (PEFT/LoRA) to improve accuracy.
- Compare performance across zero-shot and fine-tuned models.
- Produce interpretable outputs for clinical and research use.
- Maintain full traceability through documentation, notebooks, and structured source code.

---

2025-P7-Multimodal-Emotion-Recognition/
│
├── docs/                 # Design documents, functional diagram, checkpoints, Gantt, personas
├── notebooks/            # Analysis, data prep, zero-shot evaluation, fine-tuning
├── data/                 # Raw and processed datasets (empty due to large files, not versioned)
├── .gitignore
└── README.md

---

## Team

- **Amir Masoud Almasi - s337006**  
- **Parastoo Alavi - s340942** 
- **Ashkan Shafiei - s342583**
